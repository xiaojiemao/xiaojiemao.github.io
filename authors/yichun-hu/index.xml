<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yichun Hu | Xiaojie Mao</title>
    <link>https://xiaojiemao.github.io/authors/yichun-hu/</link>
      <atom:link href="https://xiaojiemao.github.io/authors/yichun-hu/index.xml" rel="self" type="application/rss+xml" />
    <description>Yichun Hu</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 18 Nov 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://xiaojiemao.github.io/img/icon-192.png</url>
      <title>Yichun Hu</title>
      <link>https://xiaojiemao.github.io/authors/yichun-hu/</link>
    </image>
    
    <item>
      <title>Fast Rates for Contextual Linear Optimization</title>
      <link>https://xiaojiemao.github.io/publication/contextual-linear/</link>
      <pubDate>Thu, 18 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://xiaojiemao.github.io/publication/contextual-linear/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: Incorporating side observations in decision making can reduce uncertainty and boost performance, but it also requires we tackle a potentially complex predictive relationship. While one may use off-the-shelf machine learning methods to separately learn a predictive model and plug it in, a variety of recent methods instead integrate estimation and optimization by fitting the model to directly optimize downstream decision performance. Surprisingly, in the case of contextual linear optimization, we show that the naive plug-in approach actually achieves regret convergence rates that are significantly faster than methods that directly optimize downstream decision performance. We show this by leveraging the fact that specific problem instances do not have arbitrarily bad near-dual-degeneracy. While there are other pros and cons to consider as we discuss and illustrate numerically, our results highlight a nuanced landscape for the enterprise to integrate estimation and optimization. Our results are overall positive for practice: predictive models are easy and fast to train using existing tools, simple to interpret, and, as we show, lead to decisions that perform very well.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Smooth Contextual Bandits: Bridging the Parametric and Non-differentiable Regret Regimes</title>
      <link>https://xiaojiemao.github.io/publication/smooth-bandit/</link>
      <pubDate>Wed, 17 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://xiaojiemao.github.io/publication/smooth-bandit/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: We study a nonparametric contextual bandit problem where the expected reward functions belong to a HÃ¶lder class with smoothness parameter $\beta$. We show how this interpolates between two extremes that were previously studied in isolation: non-differentiable bandits ($\beta \le 1$), where rate-optimal regret is achieved by running separate non-contextual bandits in different context regions, and parametric-response bandits ($\beta = \infty$), where rate-optimal regret can be achieved with minimal or no exploration due to infinite extrapolatability. We develop a novel algorithm that carefully adjusts to all smoothness settings and we prove its regret is rate-optimal by establishing matching upper and lower bounds, recovering the existing results at the two extremes. In this sense, our work bridges the gap between the existing literature on parametric and non-differentiable contextual bandit problems and between bandit algorithms that exclusively use global or local information, shedding light on the crucial interplay of complexity and regret in contextual bandits.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
