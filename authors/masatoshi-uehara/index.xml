<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Masatoshi Uehara | Xiaojie Mao</title>
    <link>https://xiaojiemao.github.io/authors/masatoshi-uehara/</link>
      <atom:link href="https://xiaojiemao.github.io/authors/masatoshi-uehara/index.xml" rel="self" type="application/rss+xml" />
    <description>Masatoshi Uehara</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 26 Mar 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://xiaojiemao.github.io/img/icon-192.png</url>
      <title>Masatoshi Uehara</title>
      <link>https://xiaojiemao.github.io/authors/masatoshi-uehara/</link>
    </image>
    
    <item>
      <title>Causal Inference Under Unmeasured Confounding With Negative Controls: A Minimax Learning Approach</title>
      <link>https://xiaojiemao.github.io/publication/negative-control/</link>
      <pubDate>Fri, 26 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://xiaojiemao.github.io/publication/negative-control/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: We study the estimation of causal parameters when not all confounders are
observed and instead negative controls are available. Recent work has shown how
these can enable identification and efficient estimation via two so-called
bridge functions. In this paper, we tackle the primary challenge to causal
inference using negative controls: the identification and estimation of these
bridge functions. Previous work has relied on uniqueness and completeness
assumptions on these functions that may be implausible in practice and also
focused on their parametric estimation. Instead, we provide a new
identification strategy that avoids both uniqueness and completeness. And, we
provide a new estimators for these functions based on minimax learning
formulations. These estimators accommodate general function classes such as
reproducing Hilbert spaces and neural networks. We study finite-sample
convergence results both for estimating bridge function themselves and for the
final estimation of the causal parameter. We do this under a variety of
combinations of assumptions that include realizability and closedness
conditions on the hypothesis and critic classes employed in the minimax
estimator. Depending on how much we are willing to assume, we obtain different
convergence rates. In some cases, we show the estimate for the causal parameter
may converge even when our bridge function estimators do not converge to any
valid bridge function. And, in other cases, we show we can obtain
semiparametric efficiency.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Localized Debiased Machine Learning: Efficient Estimation of Quantile Treatment Effects, Conditional Value at Risk, and Beyond</title>
      <link>https://xiaojiemao.github.io/publication/localizeddml/</link>
      <pubDate>Mon, 30 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://xiaojiemao.github.io/publication/localizeddml/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: We consider the efficient estimation of a low-dimensional parameter in the presence of very high-dimensional nuisances that may depend on the parameter of interest. An important example is the quantile treatment effect (QTE) in causal inference, where the efficient estimation equation involves as a nuisance the conditional cumulative distribution evaluated at the quantile to be estimated. Debiased machine learning (DML) is a data-splitting approach to address the need to estimate nuisances using flexible machine learning methods that may not satisfy strong metric entropy conditions, but applying it to problems with estimand-dependent nuisances would require estimating too many nuisances to be practical. For the QTE estimation, DML requires we learn the whole conditional cumulative distribution function, which may be challenging in practice and stands in contrast to only needing to estimate just two regression functions as in the efficient estimation of average treatment effects. Instead, we propose localized debiased machine learning (LDML), a new three-way data-splitting approach that avoids this burdensome step and needs only estimate the nuisances at a single initial bad guess for the parameters. In particular, under a Frechet-derivative orthogonality condition, we show the oracle estimation equation is asymptotically equivalent to one where the nuisance is evaluated at the true parameter value and we provide a strategy to target this alternative formulation. In the case of QTE estimation, this involves only learning two binary regression models, for which many standard, time-tested machine learning methods exist. We prove that under certain lax rate conditions, our estimator has the same favorable asymptotic behavior as the infeasible oracle estimator that solves the estimating equation with the true nuisance functions.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
